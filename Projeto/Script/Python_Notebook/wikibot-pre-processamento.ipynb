{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy\n",
    "#!pip install --user -U nltk\n",
    "#nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "#!pip install unidecode\n",
    "#!pip install matplotlib\n",
    "#!pip install pyhive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import time\n",
    "import datetime\n",
    "import boto3\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.stem import RSLPStemmer\n",
    "from unidecode import unidecode\n",
    "from pyhive import hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Carrega dados refined do Hive\n",
    "#df = pd.read_csv(\"../s3_data/refined/refined.csv\", names=[\"wiki_id\", \"title\", \"timestamp\", \"user\",\"bot\"])\n",
    "\n",
    "conn = hive.Connection(host=host,port= 10000)\n",
    "dataframe = pd.read_sql(\"SELECT wiki_id, title, wiki_timestamp, wiki_user, bot FROM defined.wikidata\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"defined_new.csv\", names=[\"wiki_id\", \"title\", \"timestamp\", \"user\",\"bot\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"defined_new.csv\", names=[\"wiki_id\", \"title\", \"timestamp\", \"user\",\"bot\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema, wiki_id, type, namespace, title, comment, wiki_timestamp, wiki_user, bot, minor, patrolled, server_url, server_name, server_script_path, wiki, parsedcomment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1\n",
    "## Pré Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Remove linhas com dados nulos, transformando em minusculo e removendo aspas simples\n",
    "df.dropna(inplace=True)\n",
    "df[\"user\"] = df[\"user\"].replace({'\\'': ''}, regex=True)\n",
    "lw_text = df[\"title\"].str.lower()\n",
    "lw_text = lw_text.replace({'\\'': ''}, regex=True)\n",
    "df[\"user\"] = df[\"user\"].str.strip().replace({'^[0-9]*$': 'unknown'}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizando timestamp\n",
    "s = \"01/01/2020\"\n",
    "default_timestamp = int(datetime.datetime.strptime('01/01/2020', '%d/%m/%Y').strftime(\"%s\"))\n",
    "df[\"timestamp\"] = df[\"timestamp\"].str.strip().replace({'^((?![0-9]).)*$': default_timestamp}, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Substitui valores diferentes de booleano pela item de maior frequencia\n",
    "max_freq = df.bot.mode()[0]\n",
    "df[\"bot\"] = df[\"bot\"].str.strip().replace({'^((?!(False|True)).)*$': max_freq.strip()}, regex=True)\n",
    "#df[\"bot\"] = df[\"bot\"].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria os tokens dos titulos\n",
    "tokens =  lw_text.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizando com unicode\n",
    "tokens_uni = tokens.apply(lambda x: [unidecode(z) for z in x ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove stopwords\n",
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "stopwords.extend(['categoria','artigos', 'predefinicao','ficheiro','sobre','predefinicoes','redirecionamentos','esbocos','ligados','elemento','inexistentes','ficheiros','usuario','wikidata','paginas','wikipedia','discussao','lista'])\n",
    "stopwords = set(stopwords + list(punctuation))\n",
    "title_cleaned = tokens_uni.apply(lambda line:  [w for w in line if not w in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria coluna com os titulos tratados\n",
    "df[\"title_cleaned\"] = title_cleaned.apply(lambda line: \" \".join(line))\n",
    "df.replace(\"\", np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Cria coluna com os titulos com steamming\n",
    "def Stemming(sentence):\n",
    "    stemmer = RSLPStemmer()\n",
    "    phrase = []\n",
    "    for word in sentence:\n",
    "        phrase.append(stemmer.stem(word.lower()))\n",
    "    return phrase\n",
    "\n",
    "stemmed_list = title_cleaned.apply(lambda line: Stemming(line))\n",
    "df[\"title_stemmed\"] = stemmed_list.apply(lambda line: \" \".join(line))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria coluna com os titulos com steamming\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def Lemmatizing(sentence):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    phrase = []\n",
    "    for word in sentence:\n",
    "        phrase.append(lemmatizer.lemmatize(word.lower()))\n",
    "    return phrase\n",
    "\n",
    "lemmatized_list = title_cleaned.apply(lambda line: Lemmatizing(line))\n",
    "df[\"title_lemmatized\"] = lemmatized_list.apply(lambda line: \" \".join(line))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from boto3.dynamodb.conditions import Key, Attr\n",
    "from botocore.exceptions import ClientError\n",
    "from collections import Counter\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_bot_true = df[df.bot==True]\n",
    "title_bot_false = df[df.bot==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vec. transform com bots = true\n",
    "str_list = title_bot_true[\"title_stemmed\"].values\n",
    "vec = TfidfVectorizer()\n",
    "vec.fit(str_list)\n",
    "features_bot_true = vec.transform(str_list)\n",
    "print(features_bot_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vec. transform com bots = false\n",
    "str_list = title_bot_false[\"title_stemmed\"].values\n",
    "vec.fit(str_list)\n",
    "features_bot_false = vec.transform(str_list)\n",
    "print(features_bot_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = 'Bot','Não Bots'\n",
    "data = [features_bot_true.size, features_bot_false.size]\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "\n",
    "ax1.pie(data, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)\n",
    "\n",
    "ax1.axis('equal')\n",
    "ax1.set_title(\"Bot vs Não Bot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Palavras mais populares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bot = true\n",
    "top_N = 10\n",
    "words = title_bot_true[\"title_cleaned\"].str.cat(sep=' ').split()\n",
    "rslt = pd.DataFrame(Counter(words).most_common(top_N), columns=['Word', 'Frequency']).set_index('Word')\n",
    "print(rslt)\n",
    "rslt.plot.bar(rot=0, figsize=(16,10), width=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bot = false\n",
    "top_N = 10\n",
    "words = title_bot_false[\"title_cleaned\"].str.cat(sep=' ').split()\n",
    "rslt = pd.DataFrame(Counter(words).most_common(top_N), columns=['Word', 'Frequency']).set_index('Word')\n",
    "print(rslt)\n",
    "rslt.plot.bar(rot=0, figsize=(16,10), width=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means bot = false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum_of_squared_distances = []\n",
    "K = range(1,30)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(features_bot_false)\n",
    "    Sum_of_squared_distances.append(km.inertia_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best result cluster = 150\n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "\n",
    "#Visualização gráfica 2D     # Converte as features para 2D     \n",
    "pca = PCA(n_components=2, random_state= 0)\n",
    "reduced_features = pca.fit_transform(features_bot_false.toarray())\n",
    "\n",
    "# Converte os centros dos clusters para 2D     \n",
    "reduced_cluster_centers = pca.transform(km.cluster_centers_)\n",
    "\n",
    "#Plota gráfico 2D     \n",
    "ax.scatter(reduced_features[:,0], reduced_features[:,1], c=km.predict(features_bot_false))\n",
    "ax.scatter(reduced_cluster_centers[:, 0], reduced_cluster_centers[:,1], marker='o', s=150, edgecolor='k')\n",
    "\n",
    "#Plota números nos clusters     \n",
    "for i, c in enumerate(reduced_cluster_centers):\n",
    "    ax.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50, edgecolor='k')\n",
    "\n",
    "cluster=5\n",
    "#Adiciona informações no gráfico     \n",
    "plt.title(\"Análise de cluster k = %d\" % cluster)\n",
    "plt.xlabel('Dispersão em X')\n",
    "plt.ylabel('Dispersão em Y')\n",
    "\n",
    "\n",
    "\n",
    "#Visualização gráfica 3D \n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2,projection=\"3d\")\n",
    "\n",
    "# ax = plt.axes(projection=\"3d\") \n",
    "# Adiciona informações no gráfico     \n",
    "plt.title(\"Análise de cluster k = %d\" % cluster)\n",
    "plt.xlabel('Dispersão em X')\n",
    "plt.ylabel('Dispersão em Y')\n",
    "\n",
    "#converte dados para 3D     \n",
    "pca = PCA(n_components=3, random_state=0)\n",
    "reduced_features = pca.fit_transform(features_bot_false.toarray())\n",
    "\n",
    "#Plota dados em 3D     \n",
    "ax.scatter3D(reduced_features[:,0], reduced_features[:,1], reduced_features[:,2], marker='o', s=150, edgecolor='k', c=km.predict(features_bot_false))\n",
    "\n",
    "# Converte os centros dos clusters para 3D     \n",
    "reduced_cluster_centers = pca.transform(km.cluster_centers_)\n",
    "\n",
    "#Salva arquivo de imagem 3D     \n",
    "plt.savefig(\"imagens/grafico_cluster_k=%d\" % cluster)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means bot = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum_of_squared_distances = []\n",
    "K = range(1,30)\n",
    "for k in K:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km = km.fit(features_bot_true)\n",
    "    Sum_of_squared_distances.append(km.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=plt.figaspect(0.5))\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "\n",
    "#Visualização gráfica 2D     # Converte as features para 2D     \n",
    "pca = PCA(n_components=2, random_state= 0)\n",
    "reduced_features = pca.fit_transform(features_bot_true.toarray())\n",
    "\n",
    "# Converte os centros dos clusters para 2D     \n",
    "reduced_cluster_centers = pca.transform(km.cluster_centers_)\n",
    "\n",
    "#Plota gráfico 2D     \n",
    "ax.scatter(reduced_features[:,0], reduced_features[:,1], c=km.predict(features_bot_true))\n",
    "ax.scatter(reduced_cluster_centers[:, 0], reduced_cluster_centers[:,1], marker='o', s=150, edgecolor='k')\n",
    "\n",
    "#Plota números nos clusters     \n",
    "for i, c in enumerate(reduced_cluster_centers):\n",
    "    ax.scatter(c[0], c[1], marker='$%d$' % i, alpha=1, s=50, edgecolor='k')\n",
    "\n",
    "cluster=5\n",
    "#Adiciona informações no gráfico     \n",
    "plt.title(\"Análise de cluster k = %d\" % cluster)\n",
    "plt.xlabel('Dispersão em X')\n",
    "plt.ylabel('Dispersão em Y')\n",
    "\n",
    "\n",
    "\n",
    "#Visualização gráfica 3D \n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2,projection=\"3d\")\n",
    "\n",
    "# ax = plt.axes(projection=\"3d\") \n",
    "# Adiciona informações no gráfico     \n",
    "plt.title(\"Análise de cluster k = %d\" % cluster)\n",
    "plt.xlabel('Dispersão em X')\n",
    "plt.ylabel('Dispersão em Y')\n",
    "\n",
    "#converte dados para 3D     \n",
    "pca = PCA(n_components=3, random_state=0)\n",
    "reduced_features = pca.fit_transform(features_bot_true.toarray())\n",
    "\n",
    "#Plota dados em 3D     \n",
    "ax.scatter3D(reduced_features[:,0], reduced_features[:,1], reduced_features[:,2], marker='o', s=150, edgecolor='k', c=km.predict(features_bot_true))\n",
    "\n",
    "# Converte os centros dos clusters para 3D     \n",
    "reduced_cluster_centers = pca.transform(km.cluster_centers_)\n",
    "\n",
    "#Salva arquivo de imagem 3D     \n",
    "plt.savefig(\"imagens/grafico_cluster_k=%d\" % cluster)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
